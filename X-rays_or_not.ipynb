{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the data directory\n",
    "data_dir = Path('Desktop/covid_19/X_rays_VS_No_X_rays')\n",
    "\n",
    "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
    "train_dir = data_dir / 'train'\n",
    "\n",
    "# Path to validation directory\n",
    "val_dir = data_dir / 'val'\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = data_dir / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ('*.jpg', '*.jpeg') # the tuple of file types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Desktop/covid_19/X_rays_VS_No_X_rays/train'\n",
    "validation_data_dir = 'Desktop/covid_19/X_rays_VS_No_X_rays/val'\n",
    "test_data_dir = 'Desktop/covid_19/X_rays_VS_No_X_rays/test'\n",
    "\n",
    "nb_train_samples = 5217\n",
    "nb_validation_samples = 17\n",
    "epochs = 20\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2150 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 634 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check information about model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x168be3950>,\n",
       " <keras.layers.core.Activation at 0x168c17dd0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1666b9a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x168c179d0>,\n",
       " <keras.layers.core.Activation at 0x103921ed0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x168c43c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x168ca2290>,\n",
       " <keras.layers.core.Activation at 0x168d0e210>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x168d0ecd0>,\n",
       " <keras.layers.core.Flatten at 0x168d150d0>,\n",
       " <keras.layers.core.Dense at 0x168d15d50>,\n",
       " <keras.layers.core.Activation at 0x168d0e710>,\n",
       " <keras.layers.core.Dropout at 0x168d1bf90>,\n",
       " <keras.layers.core.Dense at 0x168d0e6d0>,\n",
       " <keras.layers.core.Activation at 0x168d0e410>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_1_input:0' shape=(None, 150, 150, 3) dtype=float32>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_5/Sigmoid:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "326/326 [==============================] - 125s 385ms/step - loss: 0.1275 - accuracy: 0.9583 - val_loss: 0.5296 - val_accuracy: 0.9375\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 119s 364ms/step - loss: 0.1262 - accuracy: 0.9856 - val_loss: 1.0960e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 119s 366ms/step - loss: 0.1034 - accuracy: 0.9911 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 129s 395ms/step - loss: 0.0420 - accuracy: 0.9915 - val_loss: 2.0153e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 128s 393ms/step - loss: 0.0876 - accuracy: 0.9927 - val_loss: 4.0225e-13 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 115s 353ms/step - loss: 0.0678 - accuracy: 0.9929 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 128s 393ms/step - loss: 0.1199 - accuracy: 0.9892 - val_loss: 6.7099e-07 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 136s 417ms/step - loss: 0.0965 - accuracy: 0.9906 - val_loss: 2.2210e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 140s 430ms/step - loss: 0.0875 - accuracy: 0.9906 - val_loss: 1.6004e-07 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 135s 415ms/step - loss: 0.0892 - accuracy: 0.9907 - val_loss: 3.7380e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 136s 417ms/step - loss: 0.0850 - accuracy: 0.9931 - val_loss: 3.2973e-08 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 139s 426ms/step - loss: 0.1248 - accuracy: 0.9923 - val_loss: 5.1027e-06 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 137s 420ms/step - loss: 0.0653 - accuracy: 0.9921 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 141s 433ms/step - loss: 0.1141 - accuracy: 0.9931 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 136s 416ms/step - loss: 0.1392 - accuracy: 0.9929 - val_loss: 9.2207e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 124s 381ms/step - loss: 0.1188 - accuracy: 0.9865 - val_loss: 1.0733e-10 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 114s 349ms/step - loss: 0.0730 - accuracy: 0.9929 - val_loss: 1.3516e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 109s 335ms/step - loss: 0.1219 - accuracy: 0.9894 - val_loss: 1.0251e-09 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 119s 365ms/step - loss: 0.1028 - accuracy: 0.9937 - val_loss: 4.5217e-16 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 136s 417ms/step - loss: 0.1263 - accuracy: 0.9884 - val_loss: 1.4791e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x169c4f550>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model`s weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 98.90%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate_generator(test_generator)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predict = model.predict_generator(test_generator,steps = nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = Path('Desktop/covid_19/X_rays_VS_No_X_rays/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to test directory\n",
    "test_dir_xrays = test_data_dir / 'xrays'\n",
    "test_dir_no_xrays = test_data_dir / 'no_xrays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all the images\n",
    "no_xrays = test_dir_no_xrays.glob('*.jpg')\n",
    "\n",
    "xrays = test_dir_xrays.glob('*.jpg')\n",
    "xrays1 = test_dir_xrays.glob('*.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test examples:  (359, 150, 150, 3)\n",
      "Total number of labels: (359, 2)\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for img in no_xrays:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (150,150))\n",
    "    if img.shape[2] ==1:\n",
    "        img = np.dstack([img, img, img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(0, num_classes=2)\n",
    "    test_data.append(img)\n",
    "    test_labels.append(label)\n",
    "                      \n",
    "for img in xrays:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (150,150))\n",
    "    if img.shape[2] ==1:\n",
    "        img = np.dstack([img, img, img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(1, num_classes=2)\n",
    "    test_data.append(img)\n",
    "    test_labels.append(label)\n",
    "    \n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"Total number of test examples: \", test_data.shape)\n",
    "print(\"Total number of labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359,)\n",
      "(359,)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_data, batch_size=16)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Original labels\n",
    "orig_test_labels = np.argmax(test_labels, axis=-1)\n",
    "\n",
    "print(orig_test_labels.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "cm  = confusion_matrix(orig_test_labels, preds)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['no_xrays', 'Xrays'], fontsize=16)\n",
    "plt.yticks(range(2), ['no_xrays', 'Xrays'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of the model is 0.00\n",
      "Precision of the model is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastasia/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision and Recall\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
